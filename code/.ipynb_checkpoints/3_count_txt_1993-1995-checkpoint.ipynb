{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\github\\\\narrative_conservatism\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### import packages\n",
    "import os, nltk, numpy as np, pandas as pd, time, textstat\n",
    "from nltk import word_tokenize\n",
    "from tqdm import tqdm\n",
    "\n",
    "##########################################################\n",
    "##################### parameter ##########################\n",
    "##########################################################\n",
    "obj_type = '10-Q'\n",
    "period_start = 1993 # included\n",
    "period_end = 1995 # included\n",
    "\n",
    "############### Set working directory to parent directory\n",
    "os.getcwd()\n",
    "# os.chdir('F:\\\\github\\\\narrative_conservatism\\\\code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read LM disctionary\n",
    "LM = pd.read_excel('..\\\\LM\\\\LoughranMcDonald_MasterDictionary_2018.xlsx', encoding = \"utf-8\")\n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "lm_neg = LM.loc[LM['Negative'] != 0]['Word'].values.tolist()\n",
    "lm_pos = LM.loc[LM['Positive'] != 0]['Word'].values.tolist()\n",
    "lm_uctt = LM.loc[LM['Uncertainty'] != 0]['Word'].values.tolist()\n",
    "lm_lit = LM.loc[LM['Litigious'] != 0]['Word'].values.tolist()\n",
    "lm_cstr = LM.loc[LM['Constraining'] != 0]['Word'].values.tolist()\n",
    "\n",
    "lm_modal1 = LM.loc[LM['Modal'] == 1]['Word'].values.tolist()\n",
    "lm_modal2 = LM.loc[LM['Modal'] == 2]['Word'].values.tolist()\n",
    "lm_modal3 = LM.loc[LM['Modal'] == 3]['Word'].values.tolist()\n",
    "\n",
    "lm_neg = [w.lower() for w in lm_neg]\n",
    "lm_pos = [w.lower() for w in lm_pos]\n",
    "lm_uctt = [w.lower() for w in lm_uctt]\n",
    "lm_lit = [w.lower() for w in lm_lit]\n",
    "lm_cstr = [w.lower() for w in lm_cstr]\n",
    "lm_modal1 = [w.lower() for w in lm_modal1]\n",
    "lm_modal2 = [w.lower() for w in lm_modal2]\n",
    "lm_modal3 = [w.lower() for w in lm_modal3]\n",
    "\n",
    "############## Read and create stop words list\n",
    "lm_stop = list()\n",
    "with open('..\\\\LM\\\\StopWords_Generic.txt', \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace('\\n', '')\n",
    "        lm_stop.append(line)\n",
    "        \n",
    "lm_stop = [w.lower() for w in lm_stop]\n",
    "\n",
    "############# Create a negation word list\n",
    "gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### Read GI disctionary\n",
    "GI_cols = ['Entry', 'Source', 'Positiv', 'Negativ']\n",
    "GI = pd.read_excel('..\\\\LM\\\\GI\\\\inquirerbasic.xls', encoding = \"utf-8\", usecols = GI_cols)\n",
    "GI = GI[(GI['Entry'].str.endswith('#1') == True) | (GI['Entry'].str.contains('#') == False)]\n",
    "GI['Entry'] = GI['Entry'].str.replace('#1','') \n",
    "\n",
    "############### Create negative, positive, uncertainty, litigious, constraining and modal word lists\n",
    "gi_neg = GI.loc[GI['Negativ'].notnull()]['Entry'].values.tolist()\n",
    "gi_pos = GI.loc[GI['Positiv'].notnull()]['Entry'].values.tolist()\n",
    "\n",
    "gi_neg = [w.lower() for w in gi_neg]\n",
    "gi_pos = [w.lower() for w in gi_pos]\n",
    "\n",
    "############### Create Henry disctionary (HENRY 2008)\n",
    "he_neg = ['negative', 'negatives', 'fail', 'fails', 'failing', 'failure', 'weak', 'weakness', 'weaknesses', 'difficult', 'difficulty', 'hurdle', 'hurdles', 'obstacle', 'obstacles', 'slump', 'slumps', 'slumping', 'slumped', 'uncertain', 'uncertainty', 'unsettled', 'unfavorable', 'downturn', 'depressed', 'disappoint', 'disappoints', 'disappointing', 'disappointed', 'disappointment', 'risk', 'risks', 'risky', 'threat', 'threats', 'penalty', 'penalties', 'down', 'decrease', 'decreases', 'decreasing', 'decreased', 'decline', 'declines', 'declining', 'declined', 'fall', 'falls', 'falling', 'fell', 'fallen', 'drop', 'drops', 'dropping', 'dropped', 'deteriorate', 'deteriorates', 'deteriorating', 'deteriorated', 'worsen', 'worsens', 'worsening', 'weaken', 'weakens', 'weakening', 'weakened', 'worse', 'worst', 'low', 'lower', 'lowest', 'less', 'least', 'smaller', 'smallest', 'shrink']\n",
    "he_pos = ['positive', 'positives', 'success', 'successes', 'successful', 'succeed', 'succeeds', 'succeeding', 'succeeded', 'accomplish', 'accomplishes', 'accomplishing', 'accomplished', 'accomplishment', 'accomplishments', 'strong', 'strength', 'strengths', 'certain', 'certainty', 'definite', 'solid', 'excellent', 'good', 'leading', 'achieve', 'achieves', 'achieved', 'achieving', 'achievement', 'achievements', 'progress', 'progressing', 'deliver', 'delivers', 'delivered', 'delivering', 'leader', 'leading', 'pleased', 'reward', 'rewards', 'rewarding', 'rewarded', 'opportunity', 'opportunities', 'enjoy', 'enjoys', 'enjoying', 'enjoyed', 'encouraged', 'encouraging', 'up', 'increase', 'increases', 'increasing', 'increased', 'rise', 'rises', 'rising', 'rose', 'risen', 'improve', 'improves', 'improving', 'improved', 'improvement', 'improvements', 'strengthen', 'strengthens', 'strengthening', 'strengthened', 'stronger', 'strongest', 'better', 'best', 'more', 'most', 'above', 'record', 'high', 'higher', 'highest', 'greater', 'greatest', 'larger', 'largest', 'grow', 'grows', 'growing', 'grew', 'grown', 'growth', 'expand', 'expands', 'expanding', 'expanded', 'expansion', 'exceed', 'exceeds', 'exceeded', 'exceeding', 'beat', 'beats', 'beating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20346"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#####################################################################\n",
    "#################### FOR ALL PROCESSED FILES LOOP ###################\n",
    "#####################################################################\n",
    "\n",
    "############# Create input txt file index\n",
    "processed = list()\n",
    "for subdir, dirs, files in os.walk(\"H:\\\\data\\\\edgar\\\\processed\\\\\" + obj_type + '\\\\' + str(period_start) + '-' + str(period_end)):\n",
    "    for file in files:\n",
    "        processed.append(os.path.join(subdir, file))\n",
    "\n",
    "len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define a function count_occurrence to count the number of words in tup that pertaining to a list \n",
    "def count_occurrence(tup, lst): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst: \n",
    "            count+= 1\n",
    "      \n",
    "    return count\n",
    "\n",
    "### Define a function count_negation to count cases where negation occurs within four or fewer words from a word identified in list.\n",
    "def count_negation(tup, lst, negation): \n",
    "    count = 0\n",
    "    for item in tup: \n",
    "        if item in lst:\n",
    "            if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "                neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "                for neighborw in neighbor:\n",
    "                    if neighborw in negation:\n",
    "                        count+= 1\n",
    "\n",
    "            if tup.index(item)-4 < 0:\n",
    "                pre = tup[0:tup.index(item)+4]\n",
    "                for prew in pre:\n",
    "                    if prew in negation:\n",
    "                        count+= 1\n",
    "                        \n",
    "            if tup.index(item)+4 > len(tup):\n",
    "                post = tup[tup.index(item)-4:len(tup)]\n",
    "                for postw in post:\n",
    "                    if postw in negation:\n",
    "                        count+= 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 20346/20346 [9:55:45<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "############ Full Text Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw = list()\n",
    "nvocab = list()\n",
    "\n",
    "n_neg = list()\n",
    "n_pos = list()\n",
    "n_neg_gi = list()\n",
    "n_pos_gi = list()\n",
    "n_neg_he = list()\n",
    "n_pos_he = list()\n",
    "n_uctt = list()\n",
    "n_lit = list()\n",
    "n_cstr = list()\n",
    "n_modal1 = list()\n",
    "n_modal2 = list()\n",
    "n_modal3 = list()\n",
    "n_negation = list()\n",
    "READ = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        # print(repr(contents))\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens = word_tokenize(contents)\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab = sorted(set(words))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a = len(words)\n",
    "        nw.append(a)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "        b = len(vocab)\n",
    "        nvocab.append(b)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg = count_occurrence(words, lm_neg)\n",
    "        n_neg.append(neg)\n",
    "        pos = count_occurrence(words, lm_pos)\n",
    "        n_pos.append(pos)\n",
    "        uctt = count_occurrence(words, lm_uctt)\n",
    "        n_uctt.append(uctt)\n",
    "        lit = count_occurrence(words, lm_lit)\n",
    "        n_lit.append(lit)\n",
    "        cstr = count_occurrence(words, lm_cstr)\n",
    "        n_cstr.append(cstr)\n",
    "        modal1 = count_occurrence(words, lm_modal1)\n",
    "        n_modal1.append(modal1)\n",
    "        modal2 = count_occurrence(words, lm_modal2)\n",
    "        n_modal2.append(modal2)\n",
    "        modal3 = count_occurrence(words, lm_modal3)\n",
    "        n_modal3.append(modal3)\n",
    "        negation = count_negation(words, lm_pos, gt_negation)\n",
    "        n_negation.append(negation)\n",
    "        \n",
    "        neg_gi = count_occurrence(words, gi_neg)\n",
    "        n_neg_gi.append(neg_gi)\n",
    "        pos_gi = count_occurrence(words, gi_pos)\n",
    "        n_pos_gi.append(pos_gi)\n",
    "        \n",
    "        neg_he = count_occurrence(words, he_neg)\n",
    "        n_neg_he.append(neg_he)\n",
    "        pos_he = count_occurrence(words, he_pos)\n",
    "        n_pos_he.append(pos_he)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read = textstat.gunning_fog(contents)\n",
    "        READ.append(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw</th>\n",
       "      <th>nvocab</th>\n",
       "      <th>n_neg</th>\n",
       "      <th>n_pos</th>\n",
       "      <th>n_neg_gi</th>\n",
       "      <th>n_pos_gi</th>\n",
       "      <th>n_neg_he</th>\n",
       "      <th>n_pos_he</th>\n",
       "      <th>n_uctt</th>\n",
       "      <th>n_lit</th>\n",
       "      <th>n_cstr</th>\n",
       "      <th>n_modal_week</th>\n",
       "      <th>n_modal_moderate</th>\n",
       "      <th>n_modal_strong</th>\n",
       "      <th>n_negation</th>\n",
       "      <th>READ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000001952-95-000005</td>\n",
       "      <td>3301</td>\n",
       "      <td>832</td>\n",
       "      <td>110</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>119</td>\n",
       "      <td>22</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>111</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>140.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0000001952-95-000006</td>\n",
       "      <td>2662</td>\n",
       "      <td>735</td>\n",
       "      <td>89</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>102</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>95.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000001952-95-000012</td>\n",
       "      <td>2768</td>\n",
       "      <td>760</td>\n",
       "      <td>95</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>108</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>108</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>159.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000001985-95-000004</td>\n",
       "      <td>6057</td>\n",
       "      <td>980</td>\n",
       "      <td>111</td>\n",
       "      <td>34</td>\n",
       "      <td>96</td>\n",
       "      <td>281</td>\n",
       "      <td>35</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>45</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>96.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000001988-95-000005</td>\n",
       "      <td>6273</td>\n",
       "      <td>1147</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>96</td>\n",
       "      <td>290</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>27</td>\n",
       "      <td>266</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>82.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20341</td>\n",
       "      <td>0001001606-95-000010</td>\n",
       "      <td>3851</td>\n",
       "      <td>791</td>\n",
       "      <td>48</td>\n",
       "      <td>15</td>\n",
       "      <td>53</td>\n",
       "      <td>168</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>69.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20342</td>\n",
       "      <td>0001001746-95-000008</td>\n",
       "      <td>1948</td>\n",
       "      <td>590</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>67</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20343</td>\n",
       "      <td>0001003017-95-000003</td>\n",
       "      <td>2698</td>\n",
       "      <td>631</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>133</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>61.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20344</td>\n",
       "      <td>0001004003-95-000003</td>\n",
       "      <td>1153</td>\n",
       "      <td>396</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20345</td>\n",
       "      <td>9999999997-05-004450</td>\n",
       "      <td>108</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20346 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     accnum    nw  nvocab  n_neg  n_pos  n_neg_gi  n_pos_gi  \\\n",
       "0      0000001952-95-000005  3301     832    110     13        80       119   \n",
       "1      0000001952-95-000006  2662     735     89      9        44        99   \n",
       "2      0000001952-95-000012  2768     760     95      8        44       108   \n",
       "3      0000001985-95-000004  6057     980    111     34        96       281   \n",
       "4      0000001988-95-000005  6273    1147    112     29        96       290   \n",
       "...                     ...   ...     ...    ...    ...       ...       ...   \n",
       "20341  0001001606-95-000010  3851     791     48     15        53       168   \n",
       "20342  0001001746-95-000008  1948     590      8     13        24        67   \n",
       "20343  0001003017-95-000003  2698     631     15     10        22       133   \n",
       "20344  0001004003-95-000003  1153     396     15      1        24        37   \n",
       "20345  9999999997-05-004450   108      70      0      0         0         1   \n",
       "\n",
       "       n_neg_he  n_pos_he  n_uctt  n_lit  n_cstr  n_modal_week  \\\n",
       "0            22        39      30    111      10            11   \n",
       "1            12        30      28    102      10            10   \n",
       "2            14        27      27    108      10            10   \n",
       "3            35        40      33     63      45            14   \n",
       "4            22        49      27    266      60            18   \n",
       "...         ...       ...     ...    ...     ...           ...   \n",
       "20341        15        37      35     23      16             4   \n",
       "20342        15        35       9      8       5             4   \n",
       "20343        15        45      21     14      18             4   \n",
       "20344         9        16       6      6       4             1   \n",
       "20345         0         0       0      1       0             0   \n",
       "\n",
       "       n_modal_moderate  n_modal_strong  n_negation    READ  \n",
       "0                     4               4           0  140.51  \n",
       "1                     3               4           0   95.49  \n",
       "2                     3               3           0  159.52  \n",
       "3                    16               8           0   96.27  \n",
       "4                     7              18           0   82.37  \n",
       "...                 ...             ...         ...     ...  \n",
       "20341                 4               6           0   69.97  \n",
       "20342                 0               4           0   28.46  \n",
       "20343                 4               2           0   61.34  \n",
       "20344                 0               0           0   39.97  \n",
       "20345                 0               0           0   37.19  \n",
       "\n",
       "[20346 rows x 17 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: full document\n",
    "d = {'accnum': accnum, 'nw': nw, 'nvocab': nvocab, 'n_neg': n_neg, 'n_pos': n_pos, 'n_neg_gi': n_neg_gi, 'n_pos_gi': n_pos_gi, 'n_neg_he': n_neg_he, 'n_pos_he': n_pos_he, 'n_uctt': n_uctt, 'n_lit': n_lit, 'n_cstr': n_cstr, \\\n",
    "     'n_modal_week': n_modal1, 'n_modal_moderate': n_modal2, 'n_modal_strong': n_modal3, 'n_negation': n_negation, 'READ': READ}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_' + obj_type + '_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "\n",
    "text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1051/1051 [05:42<00:00,  3.07it/s]\n"
     ]
    }
   ],
   "source": [
    "############ MDA and NOTE Raw Count\n",
    "accnum = list()\n",
    "\n",
    "nw_mda = list()\n",
    "nvocab_mda = list()\n",
    "\n",
    "n_neg_mda = list()\n",
    "n_pos_mda = list()\n",
    "n_uctt_mda = list()\n",
    "n_lit_mda = list()\n",
    "n_cstr_mda = list()\n",
    "n_modal1_mda = list()\n",
    "n_modal2_mda = list()\n",
    "n_modal3_mda = list()\n",
    "n_negation_mda = list()\n",
    "\n",
    "READ_mda = list()\n",
    "\n",
    "nw_note = list()\n",
    "nvocab_note = list()\n",
    "\n",
    "n_neg_note = list()\n",
    "n_pos_note = list()\n",
    "n_uctt_note = list()\n",
    "n_lit_note = list()\n",
    "n_cstr_note = list()\n",
    "n_modal1_note = list()\n",
    "n_modal2_note = list()\n",
    "n_modal3_note = list()\n",
    "n_negation_note = list()\n",
    "\n",
    "READ_note = list()\n",
    "\n",
    "############ Word Tokenization, count nword and nvocab, count negative, positive, uncertainty, litigious, constraining and modal words\n",
    "for text in tqdm(processed):\n",
    "    ############# Create an array of accession number\n",
    "    a = text.split(\"\\\\\")[6].split(\".\")[0]\n",
    "    accnum.append(a)\n",
    "    \n",
    "    ############# Read processed txt file\n",
    "    with open(text, 'r',  encoding = \"utf-8\") as file:\n",
    "        contents = file.read().replace('\\n', ' ').replace(u'\\xa0', u' ')\n",
    "        ############################## TO EXTRACT MDA AND NOTES SECTION, UNCOMMENT THIS SECTION ################################\n",
    "        try:\n",
    "            mda = contents[contents.index(\"ITEM 2.\"):contents.index(\"ITEM 3.\")]\n",
    "        except:\n",
    "            try:\n",
    "                mda = contents[contents.index(\"Item 2.\"):contents.index(\"Item 3.\")]\n",
    "            except:\n",
    "                try:\n",
    "                    mda = contents[contents.index(\"ITEM 2\"):contents.index(\"ITEM 3\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        mda = contents[contents.index(\"Item 2\"):contents.index(\"Item 3\")]\n",
    "                    except:\n",
    "                        mda = ''\n",
    "                        pass\n",
    "                    \n",
    "        try:\n",
    "            note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2.\")]\n",
    "        except:\n",
    "            try:\n",
    "                note = contents[contents.index(\"NOTES TO\"):contents.index(\"ITEM 2\")]\n",
    "            except:\n",
    "                try:\n",
    "                    note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2.\")]\n",
    "                except:\n",
    "                    try:\n",
    "                        note = contents[contents.index(\"Notes to\"):contents.index(\"Item 2\")]\n",
    "                    except:\n",
    "                        note = ''\n",
    "                        pass\n",
    "        ###########################################################################################################\n",
    "        \n",
    "        ############ Word Tokenization\n",
    "        ## Raw tokens: including punctuations, numbers etc.\n",
    "        tokens_mda = word_tokenize(mda)\n",
    "        tokens_note = word_tokenize(note)\n",
    "        \n",
    "        ####################################################################\n",
    "\n",
    "        ## Convert all words into small cases\n",
    "        ## Keep tokens that purely consist of alphabetic characters only\n",
    "        ## Delete single-character words except for 'I'\n",
    "        words_mda = [w.lower() for w in tokens_mda if w.isalpha() and len(w)>1 or w =='i']\n",
    "        words_note = [w.lower() for w in tokens_note if w.isalpha() and len(w)>1 or w =='i']\n",
    "        \n",
    "        ########### Delete words with lenth smaller than 1% and largr than 99% of the document\n",
    "        # wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "        # wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "        # words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "        vocab_mda = sorted(set(words_mda))\n",
    "        vocab_note = sorted(set(words_note))\n",
    "        \n",
    "        ########### Save text statistics\n",
    "        ##### 1. nw 2. nvocab 3. tone 4. readability\n",
    "        \n",
    "        ## 1. nw\n",
    "        a_mda = len(words_mda)\n",
    "        nw_mda.append(a_mda)\n",
    "        a_note = len(words_note)\n",
    "        nw_note.append(a_note)\n",
    "        \n",
    "        ## 2. nvocab\n",
    "#         b_mda = len(vocab_mda)\n",
    "#         nvocab_mda.append(b_mda)\n",
    "#         b_note = len(vocab_note)\n",
    "#         nvocab_note.append(b_note)\n",
    "        \n",
    "        ## 3. tone\n",
    "        neg_mda = count_occurrence(words_mda, lm_neg)\n",
    "        n_neg_mda.append(neg_mda)\n",
    "        pos_mda = count_occurrence(words_mda, lm_pos)\n",
    "        n_pos_mda.append(pos_mda)\n",
    "#         uctt_mda = count_occurrence(words_mda, lm_uctt)\n",
    "#         n_uctt_mda.append(uctt_mda)\n",
    "#         lit_mda = count_occurrence(words_mda, lm_lit)\n",
    "#         n_lit_mda.append(lit_mda)\n",
    "#         cstr_mda = count_occurrence(words_mda, lm_cstr)\n",
    "#         n_cstr_mda.append(cstr_mda)\n",
    "#         modal1_mda = count_occurrence(words_mda, lm_modal1)\n",
    "#         n_modal1_mda.append(modal1_mda)\n",
    "#         modal2_mda = count_occurrence(words_mda, lm_modal2)\n",
    "#         n_modal2_mda.append(modal2_mda)\n",
    "#         modal3_mda = count_occurrence(words_mda, lm_modal3)\n",
    "#         n_modal3_mda.append(modal3_mda)\n",
    "        negation_mda = count_negation(words_mda, lm_pos, gt_negation)\n",
    "        n_negation_mda.append(negation_mda)\n",
    "        \n",
    "        neg_note = count_occurrence(words_note, lm_neg)\n",
    "        n_neg_note.append(neg_note)\n",
    "        pos_note = count_occurrence(words_note, lm_pos)\n",
    "        n_pos_note.append(pos_note)\n",
    "#         uctt_note = count_occurrence(words_note, lm_uctt)\n",
    "#         n_uctt_note.append(uctt_note)\n",
    "#         lit_note = count_occurrence(words_note, lm_lit)\n",
    "#         n_lit_note.append(lit_note)\n",
    "#         cstr_note = count_occurrence(words_note, lm_cstr)\n",
    "#         n_cstr_note.append(cstr_note)\n",
    "#         modal1_note = count_occurrence(words_note, lm_modal1)\n",
    "#         n_modal1_note.append(modal1_note)\n",
    "#         modal2_note = count_occurrence(words_note, lm_modal2)\n",
    "#         n_modal2_note.append(modal2_note)\n",
    "#         modal3_note = count_occurrence(words_note, lm_modal3)\n",
    "#         n_modal3_note.append(modal3_note)\n",
    "        negation_note = count_negation(words_note, lm_pos, gt_negation)\n",
    "        n_negation_note.append(negation_note)\n",
    "        \n",
    "        ## 4. readability\n",
    "        read_mda = textstat.gunning_fog(mda)\n",
    "        READ_mda.append(read_mda)\n",
    "        read_note = textstat.gunning_fog(note)\n",
    "        READ_note.append(read_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of filings whose MDA and NOTES are successfully extracted: 0.5975261655566128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accnum</th>\n",
       "      <th>nw_mda</th>\n",
       "      <th>n_neg_mda</th>\n",
       "      <th>n_pos_mda</th>\n",
       "      <th>n_negation_mda</th>\n",
       "      <th>nw_note</th>\n",
       "      <th>n_neg_note</th>\n",
       "      <th>n_pos_note</th>\n",
       "      <th>n_negation_note</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000004127-20-000007</td>\n",
       "      <td>1544</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2585</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0000004457-20-000027</td>\n",
       "      <td>8667</td>\n",
       "      <td>89</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>7863</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000006281-20-000013</td>\n",
       "      <td>3067</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4851</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0000006951-20-000014</td>\n",
       "      <td>6441</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>7804</td>\n",
       "      <td>90</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0000008670-20-000007</td>\n",
       "      <td>5946</td>\n",
       "      <td>46</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>4037</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1043</td>\n",
       "      <td>0001744489-20-000046</td>\n",
       "      <td>6929</td>\n",
       "      <td>80</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>15947</td>\n",
       "      <td>196</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1044</td>\n",
       "      <td>0001748790-20-000010</td>\n",
       "      <td>4016</td>\n",
       "      <td>86</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>5728</td>\n",
       "      <td>103</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1047</td>\n",
       "      <td>0001757898-20-000003</td>\n",
       "      <td>6543</td>\n",
       "      <td>127</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>7777</td>\n",
       "      <td>119</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1049</td>\n",
       "      <td>0001772016-20-000018</td>\n",
       "      <td>2997</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>6041</td>\n",
       "      <td>74</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0001773383-20-000004</td>\n",
       "      <td>6134</td>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>7024</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>628 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accnum  nw_mda  n_neg_mda  n_pos_mda  n_negation_mda  \\\n",
       "2     0000004127-20-000007    1544         13          8               1   \n",
       "3     0000004457-20-000027    8667         89         48               0   \n",
       "4     0000006281-20-000013    3067         10         18               1   \n",
       "6     0000006951-20-000014    6441         75         39               0   \n",
       "8     0000008670-20-000007    5946         46         81               0   \n",
       "...                    ...     ...        ...        ...             ...   \n",
       "1043  0001744489-20-000046    6929         80         30               0   \n",
       "1044  0001748790-20-000010    4016         86         28               0   \n",
       "1047  0001757898-20-000003    6543        127         53               1   \n",
       "1049  0001772016-20-000018    2997         43         20               0   \n",
       "1050  0001773383-20-000004    6134         40         46               2   \n",
       "\n",
       "      nw_note  n_neg_note  n_pos_note  n_negation_note  \n",
       "2        2585          34           8                0  \n",
       "3        7863          73          36                0  \n",
       "4        4851          30          32                0  \n",
       "6        7804          90          47                0  \n",
       "8        4037          44          21                0  \n",
       "...       ...         ...         ...              ...  \n",
       "1043    15947         196          73                0  \n",
       "1044     5728         103          33                0  \n",
       "1047     7777         119          38                2  \n",
       "1049     6041          74          30                0  \n",
       "1050     7024          49          20                0  \n",
       "\n",
       "[628 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############### Create Data Frame: MDA and NOTES\n",
    "d = {'accnum': accnum, 'nw_mda': nw_mda, 'n_neg_mda': n_neg_mda, 'n_pos_mda': n_pos_mda, 'n_negation_mda': n_negation_mda, 'nw_note': nw_note, 'n_neg_note': n_neg_note, 'n_pos_note': n_pos_note, 'n_negation_note': n_negation_note, 'READ_MDA':READ_mda, 'READ_NOTE':READ_note}\n",
    "#      'nvocab_mda': nvocab_mda, 'n_uctt_mda': n_uctt_mda, 'n_lit_mda': n_lit_mda, 'n_cstr_mda': n_cstr_mda, \\\n",
    "#      'n_modal_strong_mda': n_modal1_mda, 'n_modal_moderate_mda': n_modal2_mda, 'n_modal_weak_mda': n_modal3_mda, \\\n",
    "#      'nvocab_note': nvocab_note, 'n_uctt_note': n_uctt_note, 'n_lit_note': n_lit_note, 'n_cstr_note': n_cstr_note, \\\n",
    "#      'n_modal_strong_note': n_modal1_note, 'n_modal_moderate_note': n_modal2_note, 'n_modal_weak_note': n_modal3_note}\n",
    "\n",
    "text_data = pd.DataFrame(data=d)\n",
    "print('percentage of filings whose MDA and NOTES are successfully extracted: ' + str(text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)].shape[0]/text_data.shape[0]))\n",
    "text_data = text_data[(text_data['nw_mda']!=0) & (text_data['nw_note']!=0)]\n",
    "\n",
    "text_data.to_csv('..\\\\filings\\\\text_data_section_' + str(period_start) + '-' + str(period_end) + '.csv', index=False)\n",
    "text_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################################################\n",
    "# ################### FOR SINGLE FILE INSPECTION ######################\n",
    "# #####################################################################\n",
    "\n",
    "# ############ Word Tokenization\n",
    "# ## Raw tokens: including punctuations, numbers etc.\n",
    "# with open(processed[1], 'r',  encoding = \"utf-8\") as file:\n",
    "#     contents = file.read().replace('\\n', ' ').replace('\\xa0', ' ')\n",
    "# tokens = word_tokenize(contents)\n",
    "\n",
    "# #tokens\n",
    "\n",
    "# ## Convert all words into small cases\n",
    "# ## And keep tokens that purely consist of alphabetic characters only\n",
    "# words = [w.lower() for w in tokens if w.isalpha() and len(w)>1 or w =='i']\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# # words[2500:2600]\n",
    "# # vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_occurrence(tup, lst): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst: \n",
    "#             count+= 1\n",
    "      \n",
    "#     return count\n",
    "\n",
    "# count_occurrence(words, lm_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_negation = ['no', 'not', 'none', 'neither', 'never', 'nobody'] ## Gunnel Totie, 1991, Negation in Speech and Writing\n",
    "\n",
    "# def count_negation(tup, lst, negation): \n",
    "#     count = 0\n",
    "#     for item in tup: \n",
    "#         if item in lst:\n",
    "#             if tup.index(item)-4 > 0 and tup.index(item)+4 < len(tup):\n",
    "#                 neighbor = tup[tup.index(item)-4:tup.index(item)+4]\n",
    "#                 for neighborw in neighbor:\n",
    "#                     if neighborw in negation:\n",
    "#                         count+= 1\n",
    "\n",
    "#             if tup.index(item)-4 < 0:\n",
    "#                 pre = tup[0:tup.index(item)+4]\n",
    "#                 for prew in pre:\n",
    "#                     if prew in negation:\n",
    "#                         count+= 1\n",
    "                        \n",
    "#             if tup.index(item)+4 > len(tup):\n",
    "#                 post = tup[tup.index(item)-4:len(tup)]\n",
    "#                 for postw in post:\n",
    "#                     if postw in negation:\n",
    "#                         count+= 1\n",
    "#     return count\n",
    "\n",
    "# count_negation(words, lm_pos, gt_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########### Winsorize words with lenth smaller than 1% and largr than 99% of the document\n",
    "# wordlen99 = np.quantile([len(w) for w in words], 0.99)\n",
    "# wordlen1 = np.quantile([len(w) for w in words], 0.01)\n",
    "# words = [w for w in words if len(w)<wordlen99 and len(w)>wordlen1]\n",
    "# vocab = sorted(set(words))\n",
    "\n",
    "# vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### See the most common 20 words\n",
    "# fdist = nltk.FreqDist(words)\n",
    "# fdist.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
